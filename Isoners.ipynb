{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2935d5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "\n",
    "# ---------- 1. Load ----------\n",
    "train = pd.read_csv('train.csv', sep = ';')\n",
    "test  = pd.read_csv('test.csv', sep = ';')\n",
    "sample_sub = pd.read_csv('sample_submission.csv', sep = ',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f7c8153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 2. Aggregate target to model-season ----------\n",
    "# If weekly_demand exists per row: aggregate per id (model-season)\n",
    "# Some datasets already have multiple rows per id (weeks); we aggregate\n",
    "agg_target = train.groupby('ID', as_index=False)['weekly_demand'].sum().rename(columns={'weekly_demand':'demand'})\n",
    "# merge demand into train-level metadata (pick unique per id rows)\n",
    "meta = train.drop_duplicates(subset=['ID'])  # if each id has many weekly rows, keep one row for static metadata\n",
    "meta = meta.merge(agg_target, on='ID', how='left')\n",
    "\n",
    "# For test, we need meta rows for each id in sample_submission (use test.csv unique rows)\n",
    "test_meta = test.drop_duplicates(subset=['ID']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb3ca937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 3. Parse / extract image embedding vector ----------\n",
    "# image_embedding often stored as string like \"[0.12, 0.34, ...]\"\n",
    "def parse_embedding(s):\n",
    "    try:\n",
    "        return np.fromstring(s.strip(\"[]\"), sep=',')\n",
    "    except:\n",
    "        return np.array([])\n",
    "\n",
    "# build matrix for PCA if embeddings exist\n",
    "if 'image_embedding' in meta.columns and meta['image_embedding'].notnull().any():\n",
    "    emb_train = np.vstack(meta['image_embedding'].fillna('[]').apply(parse_embedding).values)\n",
    "    emb_test = np.vstack(test_meta['image_embedding'].fillna('[]').apply(parse_embedding).values)\n",
    "    # Some rows might have varied length or empty; handle by padding/trimming\n",
    "    # Find max dim\n",
    "    dim = max(emb_train.shape[1], emb_test.shape[1])\n",
    "    def pad_rows(mat, dim):\n",
    "        n, d = mat.shape\n",
    "        if d < dim:\n",
    "            pad = np.zeros((n, dim - d))\n",
    "            return np.hstack([mat, pad])\n",
    "        return mat[:, :dim]\n",
    "    emb_train = pad_rows(emb_train, dim)\n",
    "    emb_test  = pad_rows(emb_test, dim)\n",
    "    # PCA reduce to 8 comps\n",
    "    pca = PCA(n_components=8, random_state=42)\n",
    "    emb_all = np.vstack([emb_train, emb_test])\n",
    "    pca.fit(emb_all)\n",
    "    emb_train_p = pca.transform(emb_train)\n",
    "    emb_test_p  = pca.transform(emb_test)\n",
    "    for i in range(emb_train_p.shape[1]):\n",
    "        meta[f'emb_pca_{i}'] = emb_train_p[:, i]\n",
    "        test_meta[f'emb_pca_{i}'] = emb_test_p[:, i]\n",
    "else:\n",
    "    # no embeddings available\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00a73a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 4. Feature engineering ----------\n",
    "def feature_engineering(df):\n",
    "    X = df.copy()\n",
    "    # Numeric features (ensure no NaN)\n",
    "    numeric_feats = ['price','life_cycle_length','num_stores','num_sizes','production']\n",
    "    for c in numeric_feats:\n",
    "        if c in X.columns:\n",
    "            X[c] = pd.to_numeric(X[c], errors='coerce').fillna(0)\n",
    "    # ratios and interactions\n",
    "    if 'production' in X.columns and 'weekly_sales' in X.columns:\n",
    "        # aggregated weekly_sales per id might be present — but if not, ignore\n",
    "        X['sales_to_prod'] = X['weekly_sales'].fillna(0) / (X['production'].replace(0, np.nan).fillna(1))\n",
    "    X['stores_x_sizes'] = X.get('num_stores',0) * X.get('num_sizes',0)\n",
    "    # date/season encodings: use id_season, year, num_week_iso if available\n",
    "    if 'year' in X.columns:\n",
    "        X['year'] = X['year'].fillna(-1).astype(int)\n",
    "    # simple encoding of categorical features counts\n",
    "    cat_cols = ['aggregated_family','family','category','fabric','color_name','length_type',\n",
    "                'silhouette_type','waist_type','sleeve_length_type','print_type','archetype','moment','ocassion']\n",
    "    for c in cat_cols:\n",
    "        if c in X.columns:\n",
    "            X[c] = X[c].fillna('Unknown')\n",
    "    # For simplicity return subset\n",
    "    keep = ['ID','id_season','demand'] if 'demand' in X.columns else ['ID','id_season']\n",
    "    keep += ['price','life_cycle_length','num_stores','num_sizes','production','stores_x_sizes','sales_to_prod']\n",
    "    # add emb pca columns if present\n",
    "    keep += [col for col in X.columns if col.startswith('emb_pca_')]\n",
    "    # add categorical cols\n",
    "    keep += [c for c in cat_cols if c in X.columns]\n",
    "    # filter duplicates\n",
    "    keep = [c for c in keep if c in X.columns]\n",
    "    return X[keep]\n",
    "\n",
    "train_fe = feature_engineering(meta)\n",
    "test_fe  = feature_engineering(test_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1b7b847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 5. Prepare modeling matrices ----------\n",
    "# Label encode categoricals for LightGBM (or use category param)\n",
    "cat_cols = [c for c in train_fe.columns if train_fe[c].dtype=='object']\n",
    "le_map = {}\n",
    "for c in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    vals = list(train_fe[c].astype(str).unique()) + list(test_fe[c].astype(str).unique())\n",
    "    le.fit(vals)\n",
    "    train_fe[c] = le.transform(train_fe[c].astype(str))\n",
    "    test_fe[c]  = le.transform(test_fe[c].astype(str))\n",
    "    le_map[c] = le\n",
    "\n",
    "# Define X,y\n",
    "target_col = 'demand'\n",
    "X = train_fe.drop(columns=[c for c in ['ID','id_season','demand'] if c in train_fe.columns])\n",
    "y = train_fe[target_col].fillna(0).values\n",
    "X_test = test_fe.drop(columns=[c for c in ['ID','id_season','demand'] if c in test_fe.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bdac91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 6. Custom asymmetric squared loss for LightGBM ----------\n",
    "# Penalize underestimation more strongly. Set alpha > 1 for underestimation weight.\n",
    "alpha = 2.0  # e.g., underestimates counted 2x worse\n",
    "\n",
    "def asymmetric_squared_obj(preds, dataset):\n",
    "    # preds: raw predictions (not transformed)\n",
    "    labels = dataset.get_label()\n",
    "    resid = preds - labels\n",
    "    # weight: alpha when preds < labels (underestimate), else 1\n",
    "    w = np.where(resid < 0, alpha, 1.0)\n",
    "    grad = 2.0 * w * resid\n",
    "    hess = 2.0 * w\n",
    "    return grad, hess\n",
    "\n",
    "def asymmetric_eval(preds, dataset):\n",
    "    labels = dataset.get_label()\n",
    "    resid = preds - labels\n",
    "    # asymmetric squared error\n",
    "    w = np.where(resid < 0, alpha, 1.0)\n",
    "    loss = np.mean(w * (resid**2))\n",
    "    return 'asym_mse', loss, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca9aa96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000483 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2851\n",
      "[LightGBM] [Info] Number of data points in the train set: 7199, number of used features: 25\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l2: 7.40005e+07\tvalid_0's asym_mse: 1.19309e+08\n",
      "[200]\tvalid_0's l2: 7.09847e+07\tvalid_0's asym_mse: 1.14132e+08\n",
      "Early stopping, best iteration is:\n",
      "[169]\tvalid_0's l2: 7.09218e+07\tvalid_0's asym_mse: 1.13155e+08\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000398 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2857\n",
      "[LightGBM] [Info] Number of data points in the train set: 7307, number of used features: 25\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l2: 9.41911e+07\tvalid_0's asym_mse: 1.22634e+08\n",
      "Early stopping, best iteration is:\n",
      "[35]\tvalid_0's l2: 7.96851e+07\tvalid_0's asym_mse: 1.30003e+08\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000339 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2834\n",
      "[LightGBM] [Info] Number of data points in the train set: 5180, number of used features: 25\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l2: 5.74689e+07\tvalid_0's asym_mse: 8.97778e+07\n",
      "Early stopping, best iteration is:\n",
      "[68]\tvalid_0's l2: 5.72624e+07\tvalid_0's asym_mse: 9.3586e+07\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n"
     ]
    }
   ],
   "source": [
    "# ---------- 7. Time-aware CV and training ----------\n",
    "# Use GroupKFold on id_season to avoid leakage (train on earlier seasons) \n",
    "groups = train_fe['id_season'].values # order seasons chronologically if possible; here GroupKFold as simple option \n",
    "gkf = GroupKFold(n_splits=3)\n",
    "\n",
    "def lgb_custom_obj(y_true, y_pred):\n",
    "    resid = y_pred - y_true\n",
    "    w = np.where(resid < 0, alpha, 1.0)\n",
    "    grad = 2.0 * w * resid\n",
    "    hess = 2.0 * w\n",
    "    return grad, hess\n",
    "\n",
    "def lgb_custom_eval(y_true, y_pred):\n",
    "    resid = y_pred - y_true\n",
    "    w = np.where(resid < 0, alpha, 1.0)\n",
    "    loss = np.mean(w * (resid**2))\n",
    "    return \"asym_mse\", loss, False\n",
    "\n",
    "oof = np.zeros(len(X))\n",
    "preds_test = np.zeros(len(X_test))\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y, groups=groups)):\n",
    "\n",
    "    model = LGBMRegressor(\n",
    "        n_estimators=5000,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=31,\n",
    "        min_child_samples=50,\n",
    "        feature_fraction=0.8,\n",
    "        subsample=0.8,\n",
    "        subsample_freq=5,\n",
    "        objective=lgb_custom_obj,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X.iloc[tr_idx], y[tr_idx],\n",
    "        eval_set=[(X.iloc[val_idx], y[val_idx])],\n",
    "        eval_metric=lgb_custom_eval,\n",
    "        callbacks=[\n",
    "            early_stopping(stopping_rounds=100),\n",
    "            log_evaluation(100)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    oof[val_idx] = model.predict(X.iloc[val_idx])\n",
    "    preds_test += model.predict(X_test) / gkf.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39d4996f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 8. Simple post-processing ----------\n",
    "# Predictions must be non-negative and possibly within [0,1] if dataset is scaled 0-1\n",
    "preds_test = np.maximum(0, preds_test)\n",
    "# If targets are scaled 0-1 in the dataset, test predictions are in same scale. If you want to\n",
    "# add a family-level shrinkage: multiply by small factor if overpredicting historically — optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04cffaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID', 'Production'], dtype='object')\n",
      "Wrote submission.csv\n"
     ]
    }
   ],
   "source": [
    "# ---------- 9. Create submission ----------\n",
    "submission = sample_sub.copy()\n",
    "print(submission.columns)\n",
    "\n",
    "# Build dataframe with test ids and preds\n",
    "test_ids = test_fe['ID'].values\n",
    "pred_df = pd.DataFrame({'ID': test_ids, 'demand': preds_test})\n",
    "\n",
    "# Ensure columns exist\n",
    "if 'ID' not in submission.columns:\n",
    "    raise ValueError(\"sample_submission.csv must contain column 'ID'\")\n",
    "\n",
    "# Merge predictions with sample_submission\n",
    "submission = submission.merge(pred_df, on='ID', how='left')\n",
    "\n",
    "# Fill any missing predictions with 0\n",
    "submission['Production'] = submission['demand'].fillna(0)\n",
    "\n",
    "# Save\n",
    "submission[['ID','Production']].to_csv('submission.csv', index=False)\n",
    "print('Wrote submission.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
